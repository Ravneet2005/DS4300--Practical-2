{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-03-17T20:06:28.878063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# sample data\n",
    "fake = Faker()\n",
    "payload = [{\"song\": fake.sentence(), \"artist\": fake.name(), \"year\": fake.year()} for _ in range(1000)]\n",
    "\n",
    "# embeddings\n",
    "embeddings = []\n",
    "for item in payload:\n",
    "    text = f\"{item['artist']} {item['song']}\"\n",
    "    embedding = embedding_model.encode(text)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# quadrant client\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "my_collection = \"song_collection\"\n",
    "\n",
    "# check if collection exists, if not, create it\n",
    "try:\n",
    "    client.get_collection(collection_name=my_collection)\n",
    "    print(f\"Collection '{my_collection}' already exists. Skipping creation.\")\n",
    "except Exception as e:\n",
    "    print(f\"Collection '{my_collection}' does not exist. Creating a new one.\")\n",
    "    client.create_collection(\n",
    "        collection_name=my_collection,\n",
    "        vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE)  \n",
    "    )\n",
    "\n",
    "# upsert data (embedding vectors and payload) into the Qdrant collection\n",
    "index = list(range(len(payload)))\n",
    "client.upsert(\n",
    "    collection_name=my_collection,\n",
    "    points=models.Batch(\n",
    "        ids=index,\n",
    "        vectors=embeddings.tolist(),\n",
    "        payloads=payload\n",
    "    )\n",
    ")\n",
    "\n",
    "# semantic search using a query\n",
    "query = \"What song was released in 1975?\"\n",
    "query_embedding = embedding_model.encode(query)\n",
    "\n",
    "search_result = client.search(\n",
    "    collection_name=my_collection,\n",
    "    query_vector=query_embedding.tolist(),\n",
    "    limit=10  # limit the results to top 10 similar items\n",
    ")\n",
    "\n",
    "print(\"Search Results from Qdrant:\")\n",
    "for hit in search_result:\n",
    "    print(f\"ID: {hit.id}, Score: {hit.score}, Payload: {hit.payload}\")\n",
    "\n",
    "# Mistral 7B via Ollama for a query-based response\n",
    "response = ollama.chat(model=\"mistral-7b\", messages=[{\"role\": \"user\", \"content\": query}])\n",
    "\n",
    "print(\"\\nResponse from Mistral 7B (via Ollama):\")\n",
    "print(response['text'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4c94797ef4968e55"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d94fa96ca645e7a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
