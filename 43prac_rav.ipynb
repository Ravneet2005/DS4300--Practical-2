{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7fa150-7b49-4765-84c7-25e3c2ef96c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import chromadb\n",
    "import ollama\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb8a2ce-a7b7-42a5-8565-48595ae9fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PDF_PATH = \"/Users/avneetsoni/Desktop/ds4300/data/sample.pdf\"\n",
    "CHUNK_SIZES = [200, 500, 1000]\n",
    "CHUNK_OVERLAPS = [0, 50, 100]\n",
    "EMBEDDING_MODEL = \"hkunlp/instructor-xl\" \n",
    "TOP_K = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09e9fd9d-6f26-4215-a37e-eef1d6299fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.HttpClient(host=\"localhost\", port=8000)  \n",
    "collection = chroma_client.get_or_create_collection(name=\"notes\")\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e315b27b-908a-47ae-ae20-00d9f6476370",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"Chunk text into smaller pieces with overlapping.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(tokens), chunk_size - overlap):\n",
    "        chunk = tokens[i:i + chunk_size]\n",
    "        chunks.append(encoding.decode(chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def embed_and_store_chunks(chunks, chunk_size, overlap):\n",
    "    \"\"\"Generate embeddings and store them in ChromaDB.\"\"\"\n",
    "    embeddings = embedding_model.encode(chunks, normalize_embeddings=True)\n",
    "\n",
    "    # Store in ChromaDB\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        collection.add(\n",
    "            ids=[f\"chunk_{chunk_size}_{overlap}_{i}\"],\n",
    "            documents=[chunk],\n",
    "            embeddings=[embedding.tolist()],\n",
    "            metadatas=[{\"chunk_size\": chunk_size, \"overlap\": overlap}],\n",
    "        )\n",
    "\n",
    "\n",
    "def retrieve_relevant_chunks(query, top_k=TOP_K):\n",
    "    \"\"\"Retrieve the top K most relevant chunks from ChromaDB.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query], normalize_embeddings=True)[0]\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding.tolist()],\n",
    "        n_results=top_k,\n",
    "    )\n",
    "\n",
    "    return results[\"documents\"][0] if results[\"documents\"] else []\n",
    "\n",
    "def generate_response(model,context, query):\n",
    "    \"\"\"Generate a response using Llama 2 (local Ollama).\"\"\"\n",
    "    prompt = f\"Given the context:\\n{context}\\n\\nAnswer the following question:\\n{query}\"\n",
    "    \n",
    "    response = ollama.chat(model=model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    \n",
    "    return response[\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "189a277c-4189-437c-b4c8-28e16c81da16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 2212 characters from PDF\n",
      "\n",
      "Processing chunk_size=200, overlap=0\n",
      " - Stored 3 chunks in ChromaDB in 18.15 sec\n",
      "\n",
      "Processing chunk_size=200, overlap=50\n",
      " - Stored 4 chunks in ChromaDB in 24.11 sec\n",
      "\n",
      "Processing chunk_size=200, overlap=100\n",
      " - Stored 5 chunks in ChromaDB in 26.20 sec\n",
      "\n",
      "Processing chunk_size=500, overlap=0\n",
      " - Stored 1 chunks in ChromaDB in 22.80 sec\n",
      "\n",
      "Processing chunk_size=500, overlap=50\n",
      " - Stored 2 chunks in ChromaDB in 29.08 sec\n",
      "\n",
      "Processing chunk_size=500, overlap=100\n",
      " - Stored 2 chunks in ChromaDB in 26.01 sec\n",
      "\n",
      "Processing chunk_size=1000, overlap=0\n",
      " - Stored 1 chunks in ChromaDB in 25.13 sec\n",
      "\n",
      "Processing chunk_size=1000, overlap=50\n",
      " - Stored 1 chunks in ChromaDB in 20.87 sec\n",
      "\n",
      "Processing chunk_size=1000, overlap=100\n",
      " - Stored 1 chunks in ChromaDB in 20.04 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    text = extract_text_from_pdf(PDF_PATH)\n",
    "    print(f\"Extracted {len(text)} characters from PDF\\n\")\n",
    "\n",
    "    for chunk_size in CHUNK_SIZES:\n",
    "        for overlap in CHUNK_OVERLAPS:\n",
    "            print(f\"Processing chunk_size={chunk_size}, overlap={overlap}\")\n",
    "            chunks = chunk_text(text, chunk_size, overlap)\n",
    "\n",
    "            start_time = time.time()\n",
    "            embed_and_store_chunks(chunks, chunk_size, overlap)\n",
    "            print(f\" - Stored {len(chunks)} chunks in ChromaDB in {time.time() - start_time:.2f} sec\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b82315c8-8464-4c45-8509-822b3421878d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retrieved Context Chunks:\n",
      " - �  Indexing: Speeds up searches by organizing data. \n",
      " ○  Storage Control: Optimizes data layout for efficiency. \n",
      " ○  Column vs. Row Storage: Column-oriented storage benefits analytics; \n",
      " row-oriented ...\n",
      "\n",
      " -  Module 3 – Moving Beyond the Relational Model \n",
      " ●  Benefits of the relational model \n",
      " ○  Relational databases use SQL which is a widely adopted standard \n",
      " ○  ACID compliance ensures transactions are ...\n",
      "\n",
      " -  Module 3 – Moving Beyond the Relational Model \n",
      " ●  Benefits of the relational model \n",
      " ○  Relational databases use SQL which is a widely adopted standard \n",
      " ○  ACID compliance ensures transactions are ...\n",
      "\n",
      "\n",
      "Llama 2 Response:\n",
      " Indexing is a technique used in databases to speed up searches by organizing data. It involves creating a data structure that allows for faster access to specific data within a larger dataset. This is achieved by creating an index, which is a data structure that contains a copy of the data from one or more tables, along with links to the corresponding records in the underlying table(s).\n",
      "\n",
      "When a query is executed, the database can quickly locate the relevant data using the index, rather than having to search through the entire dataset. This can significantly improve the performance of queries that are frequently executed, such as those that filter or sort data.\n",
      "\n",
      "There are different types of indexing techniques, including:\n",
      "\n",
      "1. B-tree indexing: This is a common type of indexing used in relational databases. It uses a tree-like data structure to organize the data and allow for efficient searching.\n",
      "2. Hash indexing: This technique uses a hash function to map the data to a specific location in memory, allowing for fast lookups.\n",
      "3. Full-text indexing: This technique is used for text search applications and allows for fast searching of unstructured or semi-structured data.\n",
      "4. Spatial indexing: This technique is used for storing and querying geospatial data, such as locations on a map.\n",
      "5. Multimedia indexing: This technique is used for storing and querying multimedia data, such as images and videos.\n",
      "\n",
      "In summary, indexing is a powerful technique that can significantly improve the performance of databases by organizing data in a way that allows for faster access to specific data.\n"
     ]
    }
   ],
   "source": [
    "# Query Testing\n",
    "query = \"what is indexing\"\n",
    "retrieved_chunks = retrieve_relevant_chunks(query)\n",
    "\n",
    "print(\"\\nRetrieved Context Chunks:\")\n",
    "for chunk in retrieved_chunks:\n",
    "    print(f\" - {chunk[:200]}...\\n\")\n",
    "\n",
    "# Generate response using Llama 2\n",
    "if retrieved_chunks:\n",
    "    response = generate_response(\"llama2\",\"\\n\".join(retrieved_chunks), query)\n",
    "    print(\"\\nLlama 2 Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd5b1679-e66c-42fc-8ebe-c4f9ed58b897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mistral Response:\n",
      "  Indexing, in the context of databases, refers to a method used to speed up searches by organizing data in a more efficient manner. Instead of reading every record or row in a table to find a specific piece of information, an index allows the database system to quickly locate the necessary data based on the index itself. This process improves the performance and speed of search operations.\n"
     ]
    }
   ],
   "source": [
    "# Generate response using Mistral\n",
    "if retrieved_chunks:\n",
    "    response = generate_response(\"mistral\",\"\\n\".join(retrieved_chunks), query)\n",
    "    print(\"\\nMistral Response:\\n\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
